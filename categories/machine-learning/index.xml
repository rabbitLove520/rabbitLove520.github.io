<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Machine Learning - 分类 - 猪爱兔的网站</title><link>https://rabbitLove520.github.io/categories/machine-learning/</link><description>Machine Learning - 分类 - 猪爱兔的网站</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 08 Oct 2024 14:00:00 +0000</lastBuildDate><atom:link href="https://rabbitLove520.github.io/categories/machine-learning/" rel="self" type="application/rss+xml"/><item><title>最小二乘法和线性回归</title><link>https://rabbitLove520.github.io/least_square/</link><pubDate>Tue, 08 Oct 2024 14:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://rabbitLove520.github.io/least_square/</guid><description><![CDATA[<h2 id="最小二乘法">最小二乘法</h2>
<p>早在19世纪,勒让德就认为让&quot;误差的平方和最小&quot;估计出来的模型是最接近真实情形的。<br>
按照勒让德的最佳原则,于是就是求:<br>
$$
\text{L} = \sum_{i=1}^{n} \left( y_i - f(x_i) \right)^2
$$
这个目标函数取得最小值时的函数参数,这就是最小二乘法的思想想,所谓&quot;二乘&quot;就是平方的意思。从这里我们可以看到,<strong>最小二乘法其实
就是用来做函数拟合的一种思想</strong>。<br>
至于怎么求出具体的参数那就是另外一个问题了,理论上可以用导数法、几何法,工程上可以用<strong>梯度下降法</strong>。下面以最常用的线性回归为
例进行推导和理解。<br>
在<strong>机器学习</strong>中用于回归问题的损失函数(Loss Function)是均方误差(MSE)：
$$
\text{L} = \frac{1}{2n} \sum_{i=1}^{n} \left( y_i - f(x_i) \right)^2
$$
其实就是多了个1/2n。</p>]]></description></item></channel></rss>
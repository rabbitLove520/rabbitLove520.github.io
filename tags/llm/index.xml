<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Llm - 标签 - 猪爱兔的网站</title><link>https://rabbitLove520.github.io/tags/llm/</link><description>Llm - 标签 - 猪爱兔的网站</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 01 Dec 2024 20:00:00 +0000</lastBuildDate><atom:link href="https://rabbitLove520.github.io/tags/llm/" rel="self" type="application/rss+xml"/><item><title>Attention机制学习</title><link>https://rabbitLove520.github.io/llm_learning/</link><pubDate>Sun, 01 Dec 2024 20:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://rabbitLove520.github.io/llm_learning/</guid><description><![CDATA[<p></p>
<!-- more -->
<h2 id="q-k-v">Q K V</h2>
<p>在深度学习中，很多 LLM 的训练都使用 Transformer 架构，而在 Transformer 架构中计算的过程涉及到的最关键的就是注意力，它是整个过程中重要的基础。注意力抽象出了 3 个重要的概念，在计算过程中对应着 3 个矩阵，如下所示：</p>]]></description></item></channel></rss>
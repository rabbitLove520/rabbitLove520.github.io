<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Triton写简单算子 - 猪爱兔的网站</title><meta name=Description content="Javascript NodeJs C# software developer"><meta property="og:url" content="https://rabbitLove520.github.io/triton_learning/">
<meta property="og:site_name" content="猪爱兔的网站"><meta property="og:title" content="Triton写简单算子"><meta property="og:description" content="
Triton 官网介绍：Triton 是一种用于并行编程的语言和编译器。它旨在提供一个基于 Python 的编程环境，以高效编写自定义 DNN 计算内核，并能够在现代 GPU 硬件上以最大吞吐量运行。
简单讲，就是可以用Python写GPU算子。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-02T14:00:00+00:00"><meta property="article:modified_time" content="2025-12-18T14:59:21+08:00"><meta property="og:image" content="https://rabbitLove520.github.io/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rabbitLove520.github.io/logo.png"><meta name=twitter:title content="Triton写简单算子"><meta name=twitter:description content="
Triton 官网介绍：Triton 是一种用于并行编程的语言和编译器。它旨在提供一个基于 Python 的编程环境，以高效编写自定义 DNN 计算内核，并能够在现代 GPU 硬件上以最大吞吐量运行。
简单讲，就是可以用Python写GPU算子。"><meta name=application-name content="pigLoveRabbit"><meta name=apple-mobile-web-app-title content="pigLoveRabbit"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://rabbitLove520.github.io/triton_learning/><link rel=prev href=https://rabbitLove520.github.io/torch_learning/><link rel=next href=https://rabbitLove520.github.io/triton_attention/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Triton写简单算子","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/rabbitLove520.github.io\/triton_learning\/"},"image":["https:\/\/rabbitLove520.github.io\/images\/Apple-Devices-Preview.png"],"genre":"posts","wordcount":4534,"url":"https:\/\/rabbitLove520.github.io\/triton_learning\/","datePublished":"2025-10-02T14:00:00+00:00","dateModified":"2025-12-18T14:59:21+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"xxxx","logo":"https:\/\/rabbitLove520.github.io\/images\/avatar.jpg"},"author":{"@type":"Person","name":"pigLoveRabbit"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=猪爱兔的网站><span class=header-title-pre><i class='far fa-kiss-wink-heart fa-fw' aria-hidden=true></i></span>pigLoveRabbit</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>所有文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/categories/documentation/>文档 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://github.com/pigLoveRabbit520 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="menu-item language" title=选择语言><i class="fa fa-globe" aria-hidden=true></i>
<select class=language-select id=language-select-desktop onchange="location=this.value"><option value=/triton_learning/ selected>简体中文</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=猪爱兔的网站><span class=header-title-pre><i class='far fa-kiss-wink-heart fa-fw' aria-hidden=true></i></span>pigLoveRabbit</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/posts/ title>所有文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/categories/documentation/ title>文档</a><a class=menu-item href=/about/ title>关于</a><a class=menu-item href=https://github.com/pigLoveRabbit520 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class=menu-item title=选择语言><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select onchange="location=this.value"><option value=/triton_learning/ selected>简体中文</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Triton写简单算子</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/rabbitLove520 title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>pigLoveRabbit</a></span>&nbsp;<span class=post-category>收录于 <a href=/categories/python/><i class="far fa-folder fa-fw" aria-hidden=true></i>Python</a>&nbsp;<a href=/categories/triton/><i class="far fa-folder fa-fw" aria-hidden=true></i>Triton</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2025-10-02>2025-10-02</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;约 4534 字&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;预计阅读 10 分钟&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#triton>Triton</a></li><li><a href=#简单的add算子>简单的Add算子</a><ul><li><a href=#关键部分解释>关键部分解释</a></li></ul></li><li><a href=#softmax算子>Softmax算子</a><ul><li><a href=#naive-softmax实现>Naive Softmax实现</a></li><li><a href=#n较小的triton版softmax>N较小的Triton版softmax</a></li><li><a href=#说明>说明</a></li></ul></li><li><a href=#为啥需要input_row_stride这样的参数>为啥需要input_row_stride这样的参数</a><ul><li><a href=#1-什么是-stride步长>1. 什么是 stride（步长）？</a></li><li><a href=#2-为什么不能假设-stride-是固定的>2. 为什么不能假设 stride 是固定的？</a><ul><li><a href=#情况一转置transpose>情况一：转置（transpose）</a></li><li><a href=#情况二切片slice>情况二：切片（slice）</a></li><li><a href=#情况三view--reshape-后的非连续张量>情况三：view / reshape 后的非连续张量</a></li></ul></li><li><a href=#3-如果不传-stride-会发生什么>3. 如果不传 stride 会发生什么？</a></li><li><a href=#4-正确做法使用实际-stride>4. 正确做法：使用实际 stride</a></li><li><a href=#5-实际例子对比>5. 实际例子对比</a></li><li><a href=#6-补充为什么-output-也需要-stride>6. 补充：为什么 output 也需要 stride？</a></li></ul></li><li><a href=#mean算子>mean算子</a><ul><li><a href=#什么时候会返回-copy>什么时候会返回 copy？</a></li><li><a href=#举个例子>举个例子</a></li><li><a href=#对比连续张量的-reshape返回-view>对比：连续张量的 reshape（返回 view）</a></li><li><a href=#总结>总结</a></li></ul></li><li><a href=#自动调优autotune>自动调优autotune</a></li></ul></nav></div></div><div class=content id=content><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/triton-logo.png data-srcset="/images/triton-logo.png, /images/triton-logo.png 1.5x, /images/triton-logo.png 2x" data-sizes=auto alt=/images/triton-logo.png title="upload successful"></p><h2 id=triton>Triton</h2><p><a href=https://triton.hyper.ai/ target=_blank rel="noopener noreffer">官网介绍</a>：Triton 是一种用于并行编程的语言和编译器。它旨在提供一个基于 Python 的编程环境，以高效编写自定义 DNN 计算内核，并能够在现代 GPU 硬件上以最大吞吐量运行。<br>简单讲，就是可以用Python写GPU算子。</p><h2 id=简单的add算子>简单的Add算子</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton.language</span> <span class=k>as</span> <span class=nn>tl</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 定义 kernel 函数，执行逐元素加法</span>
</span></span><span class=line><span class=cl><span class=nd>@triton.jit</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>add_kernel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>x_ptr</span><span class=p>,</span> <span class=n>y_ptr</span><span class=p>,</span> <span class=n>output_ptr</span><span class=p>,</span> <span class=n>n_elements</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>BLOCK_SIZE</span><span class=p>:</span> <span class=n>tl</span><span class=o>.</span><span class=n>constexpr</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 获取当前线程的 ID</span>
</span></span><span class=line><span class=cl>    <span class=n>pid</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>program_id</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 计算当前 block 内的索引</span>
</span></span><span class=line><span class=cl>    <span class=n>offsets</span> <span class=o>=</span> <span class=n>pid</span> <span class=o>*</span> <span class=n>BLOCK_SIZE</span> <span class=o>+</span> <span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 通过 masks 防止越界访问</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>offsets</span> <span class=o>&lt;</span> <span class=n>n_elements</span>
</span></span><span class=line><span class=cl>    <span class=c1># 从全局内存中加载 x 和 y 的元素</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>x_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>y_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 执行逐元素加法</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>    <span class=c1># 将结果写回到全局内存</span>
</span></span><span class=line><span class=cl>    <span class=n>tl</span><span class=o>.</span><span class=n>store</span><span class=p>(</span><span class=n>output_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>output</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>[</span><span class=n>i</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1023</span><span class=p>)]</span> <span class=c1># 特意设置了1022个元素，让128不整除</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试数据</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>empty_like</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 每个 block 的线程数</span>
</span></span><span class=line><span class=cl><span class=n>block_size</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>grid</span> <span class=o>=</span> <span class=n>triton</span><span class=o>.</span><span class=n>cdiv</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>numel</span><span class=p>(),</span> <span class=n>block_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 启动 kernel，执行逐元素加法</span>
</span></span><span class=line><span class=cl><span class=n>add_kernel</span><span class=p>[(</span><span class=n>grid</span><span class=p>,)](</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>output</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>numel</span><span class=p>(),</span> <span class=n>BLOCK_SIZE</span><span class=o>=</span><span class=n>block_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 检查结果是否正确</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>allclose</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>运行上面代码，可以得到结果：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>tensor([   2,    4,    6,  ..., 2040, 2042, 2044], device=&#39;cuda:0&#39;)
</span></span><span class=line><span class=cl>True
</span></span></code></pre></td></tr></table></div></div><h3 id=关键部分解释>关键部分解释</h3><ul><li>@triton.jit 装饰器：这个装饰器将 Python 函数 JIT 编译为 Triton kernel。函数的输入可以是张量指针、整数或编译时常量。</li><li>tl.program_id(0)：返回当前线程块的唯一 ID，类似于 CUDA 中的 blockIdx.x。</li><li>tl.arange(0, BLOCK_SIZE)：为当前线程块中的每个线程分配索引。</li><li>x.numel()表示x的元素个数，<code>triton.cdiv(x.numel(), block_size)</code>就是向上取整，保证grid数量足够。</li><li>tl.load() 和 tl.store()：用于从全局内存加载数据和将数据存储回全局内存。</li><li>mask：用来防止越界访问（即确保线程访问的数据在张量范围内）。</li></ul><h2 id=softmax算子>Softmax算子</h2><h3 id=naive-softmax实现>Naive Softmax实现</h3><p>首先，使用pytorch实现一个row-wise的naive softmax:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>naive_softmax</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Compute row-wise softmax of X using native pytorch
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    We subtract the maximum element in order to avoid overflows. Softmax is invariant to
</span></span></span><span class=line><span class=cl><span class=s2>    this shift.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># read  MN elements ; write M  elements; 读取MN元素；写M个元素</span>
</span></span><span class=line><span class=cl>    <span class=n>x_max</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=c1># read MN + M elements ; write MN elements; 读取MN+M元素；写入MN元素</span>
</span></span><span class=line><span class=cl>    <span class=n>z</span> <span class=o>=</span> <span class=n>x</span> <span class=o>-</span> <span class=n>x_max</span><span class=p>[:,</span> <span class=kc>None</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=c1># read  MN elements ; write MN elements; 读取MN元素；写入MN元素</span>
</span></span><span class=line><span class=cl>    <span class=n>numerator</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># read  MN elements ; write M  elements; 读取MN元素；写M个元素</span>
</span></span><span class=line><span class=cl>    <span class=n>denominator</span> <span class=o>=</span> <span class=n>numerator</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># read MN + M elements ; write MN elements; 读取MN M元素；写入MN元素</span>
</span></span><span class=line><span class=cl>    <span class=n>ret</span> <span class=o>=</span> <span class=n>numerator</span> <span class=o>/</span> <span class=n>denominator</span><span class=p>[:,</span> <span class=kc>None</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=c1># in total: read 5MN + 2M elements ; wrote 3MN + 2M elements;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>ret</span> <span class=c1># 共：读取5MN+2M元素；写了3MN+2M个元素</span>
</span></span></code></pre></td></tr></table></div></div><p>为什么叫 “naive”（朴素）？</p><ul><li>它使用了 多个中间张量（x_max, z, numerator, denominator, ret），每个都占用显存。</li><li>每一步都是独立的 PyTorch 操作，无法融合（kernel fusion），导致多次读写全局内存。</li><li>在 GPU 上，这会带来 较高的内存带宽压力 和 较低的计算效率。</li></ul><h3 id=n较小的triton版softmax>N较小的Triton版softmax</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span><span class=lnt>86
</span><span class=lnt>87
</span><span class=lnt>88
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton.language</span> <span class=k>as</span> <span class=nn>tl</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@triton.jit</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>softmax_kernel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ptr</span><span class=p>,</span>        <span class=c1># 输出指针</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ptr</span><span class=p>,</span>         <span class=c1># 输入指针</span>
</span></span><span class=line><span class=cl>    <span class=n>input_row_stride</span><span class=p>,</span>  <span class=c1># 输入每行的步长（以元素为单位）</span>
</span></span><span class=line><span class=cl>    <span class=n>output_row_stride</span><span class=p>,</span> <span class=c1># 输出每行的步长</span>
</span></span><span class=line><span class=cl>    <span class=n>n_cols</span><span class=p>,</span>            <span class=c1># 每行的列数 N</span>
</span></span><span class=line><span class=cl>    <span class=n>BLOCK_SIZE</span><span class=p>:</span> <span class=n>tl</span><span class=o>.</span><span class=n>constexpr</span><span class=p>,</span>  <span class=c1># 每个线程块处理的列数</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 获取当前处理的行索引</span>
</span></span><span class=line><span class=cl>    <span class=n>row_idx</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>program_id</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 计算该行在全局内存中的起始地址</span>
</span></span><span class=line><span class=cl>    <span class=n>row_start_ptr</span> <span class=o>=</span> <span class=n>input_ptr</span> <span class=o>+</span> <span class=n>row_idx</span> <span class=o>*</span> <span class=n>input_row_stride</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 使用向量化加载（如果 BLOCK_SIZE 是 128 的倍数等）</span>
</span></span><span class=line><span class=cl>    <span class=n>col_offsets</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ptrs</span> <span class=o>=</span> <span class=n>row_start_ptr</span> <span class=o>+</span> <span class=n>col_offsets</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 掩码：防止越界访问</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>col_offsets</span> <span class=o>&lt;</span> <span class=n>n_cols</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 加载输入行</span>
</span></span><span class=line><span class=cl>    <span class=n>row</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>input_ptrs</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span> <span class=n>other</span><span class=o>=-</span><span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 数值稳定：减去最大值</span>
</span></span><span class=line><span class=cl>    <span class=n>row_max</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>row</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>row_minus_max</span> <span class=o>=</span> <span class=n>row</span> <span class=o>-</span> <span class=n>row_max</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 计算 exp</span>
</span></span><span class=line><span class=cl>    <span class=n>exp_row</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>row_minus_max</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 求和</span>
</span></span><span class=line><span class=cl>    <span class=n>exp_sum</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>exp_row</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 归一化</span>
</span></span><span class=line><span class=cl>    <span class=n>softmax_output</span> <span class=o>=</span> <span class=n>exp_row</span> <span class=o>/</span> <span class=n>exp_sum</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 存储结果</span>
</span></span><span class=line><span class=cl>    <span class=n>output_row_start</span> <span class=o>=</span> <span class=n>output_ptr</span> <span class=o>+</span> <span class=n>row_idx</span> <span class=o>*</span> <span class=n>output_row_stride</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ptrs</span> <span class=o>=</span> <span class=n>output_row_start</span> <span class=o>+</span> <span class=n>col_offsets</span>
</span></span><span class=line><span class=cl>    <span class=n>tl</span><span class=o>.</span><span class=n>store</span><span class=p>(</span><span class=n>output_ptrs</span><span class=p>,</span> <span class=n>softmax_output</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>softmax</span><span class=p>(</span><span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    对输入张量 x 的最后一维执行 softmax。
</span></span></span><span class=line><span class=cl><span class=s2>    假设 x 是 2D 张量 (M, N)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>is_cuda</span><span class=p>,</span> <span class=s2>&#34;Input must be on GPU&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>dim</span><span class=p>()</span> <span class=o>==</span> <span class=mi>2</span><span class=p>,</span> <span class=s2>&#34;Only 2D tensors supported for now&#34;</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>M</span><span class=p>,</span> <span class=n>N</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 选择合适的 BLOCK_SIZE（必须是 2 的幂，且 &lt;= 4096）</span>
</span></span><span class=line><span class=cl>    <span class=n>BLOCK_SIZE</span> <span class=o>=</span> <span class=n>triton</span><span class=o>.</span><span class=n>next_power_of_2</span><span class=p>(</span><span class=n>N</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>BLOCK_SIZE</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=n>BLOCK_SIZE</span><span class=p>,</span> <span class=mi>4096</span><span class=p>)</span>  <span class=c1># Triton 限制最大 BLOCK_SIZE</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 分配输出张量</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>empty_like</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 启动内核</span>
</span></span><span class=line><span class=cl>    <span class=n>grid</span> <span class=o>=</span> <span class=p>(</span><span class=n>M</span><span class=p>,)</span>  <span class=c1># 每行一个程序</span>
</span></span><span class=line><span class=cl>    <span class=n>softmax_kernel</span><span class=p>[</span><span class=n>grid</span><span class=p>](</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=o>.</span><span class=n>stride</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span>   <span class=c1># input_row_stride</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span><span class=o>.</span><span class=n>stride</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span>  <span class=c1># output_row_stride</span>
</span></span><span class=line><span class=cl>        <span class=n>N</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>BLOCK_SIZE</span><span class=o>=</span><span class=n>BLOCK_SIZE</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试</span>
</span></span><span class=line><span class=cl><span class=n>M</span><span class=p>,</span> <span class=n>N</span> <span class=o>=</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>M</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>y_torch</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y_triton</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 检查数值误差</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Max diff:&#34;</span><span class=p>,</span> <span class=p>(</span><span class=n>y_torch</span> <span class=o>-</span> <span class=n>y_triton</span><span class=p>)</span><span class=o>.</span><span class=n>abs</span><span class=p>()</span><span class=o>.</span><span class=n>max</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>allclose</span><span class=p>(</span><span class=n>y_torch</span><span class=p>,</span> <span class=n>y_triton</span><span class=p>,</span> <span class=n>atol</span><span class=o>=</span><span class=mf>1e-5</span><span class=p>),</span> <span class=s2>&#34;Results don&#39;t match!&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;✅ Triton softmax matches PyTorch!&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=说明>说明</h3><ul><li>数值稳定性：通过减去每行最大值避免 exp 溢出。</li><li>并行性：每行由一个 Triton 程序（program）处理，利用 BLOCK_SIZE 个线程并行加载/计算该行。</li><li>内存访问：使用 mask 避免越界读写，适用于 N 不是 BLOCK_SIZE 整数倍的情况。</li><li>BLOCK_SIZE：自动选择最接近 N 的 2 的幂，但不超过 4096（Triton 的限制）。</li></ul><h2 id=为啥需要input_row_stride这样的参数>为啥需要input_row_stride这样的参数</h2><p><code>input_row_stride</code>（以及 <code>output_row_stride</code>）参数的存在，是为了让 Triton 内核能够<strong>正确处理非连续内存布局（non-contiguous tensors）</strong>，尤其是<strong>跨步（strided）张量</strong>。</p><hr><h3 id=1-什么是-stride步长>1. 什么是 stride（步长）？</h3><p>在 PyTorch（以及大多数张量库）中，一个张量在内存中是<strong>一维线性存储</strong>的，但通过 <code>shape</code> 和 <code>stride</code> 来解释其多维结构。</p><p>例如：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span>  <span class=c1># shape=(4,8)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>stride</span><span class=p>())</span>      <span class=c1># 通常是 (8, 1)</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><code>stride[0] = 8</code>：表示第 0 维（行）每增加 1，内存地址跳过 8 个元素。</li><li><code>stride[1] = 1</code>：表示第 1 维（列）每增加 1，内存地址跳过 1 个元素。</li></ul><p>所以，<code>x[i, j]</code> 对应的内存偏移是：<code>i * stride[0] + j * stride[1]</code>。</p><hr><h3 id=2-为什么不能假设-stride-是固定的>2. 为什么不能假设 stride 是固定的？</h3><p>因为 PyTorch 张量<strong>不一定是连续的</strong>！例如：</p><h4 id=情况一转置transpose>情况一：转置（transpose）</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>t</span><span class=p>()</span>  <span class=c1># shape=(8,4), stride=(1, 8) ← 注意！</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>现在 <code>y</code> 的第 0 维（原列）的 stride 是 1，第 1 维（原行）的 stride 是 8。</li><li>如果你的内核假设“每行连续存储（stride=列数）”，那么对 <code>y</code> 调用 softmax 就会出错！</li></ul><h4 id=情况二切片slice>情况二：切片（slice）</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>20</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>x</span><span class=p>[::</span><span class=mi>2</span><span class=p>,</span> <span class=p>:]</span>  <span class=c1># 取偶数行，shape=(5,20)，但 stride[0] = 40（不是20！）</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=情况三view--reshape-后的非连续张量>情况三：view / reshape 后的非连续张量</h4><p>某些 reshape 操作可能产生非连续内存布局。</p><hr><h3 id=3-如果不传-stride-会发生什么>3. 如果不传 stride 会发生什么？</h3><p>假设你写死：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>row_start_ptr</span> <span class=o>=</span> <span class=n>input_ptr</span> <span class=o>+</span> <span class=n>row_idx</span> <span class=o>*</span> <span class=n>n_cols</span>  <span class=c1># ❌ 错误！</span>
</span></span></code></pre></td></tr></table></div></div><p>这<strong>只在张量是行连续（row-major, contiguous）时才正确</strong>。一旦遇到转置或切片张量，就会：</p><ul><li>读取错误的内存位置</li><li>导致结果完全错误</li><li>甚至越界访问（segmentation fault）</li></ul><hr><h3 id=4-正确做法使用实际-stride>4. 正确做法：使用实际 stride</h3><p>通过传入 <code>x.stride(0)</code>（即第 0 维的 stride），Triton 内核可以正确计算每一行的起始地址：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>row_start_ptr</span> <span class=o>=</span> <span class=n>input_ptr</span> <span class=o>+</span> <span class=n>row_idx</span> <span class=o>*</span> <span class=n>input_row_stride</span>
</span></span></code></pre></td></tr></table></div></div><p>这样无论张量是否连续、是否转置、是否切片，都能正确访问数据。</p><hr><h3 id=5-实际例子对比>5. 实际例子对比</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>t</span><span class=p>()</span>  <span class=c1># shape=(4,2), stride=(1, 4)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 正确调用（传 stride）：</span>
</span></span><span class=line><span class=cl><span class=n>softmax</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>  <span class=c1># 内核使用 y.stride(0) == 1，正确！</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 错误实现（假设 stride = n_cols = 2）：</span>
</span></span><span class=line><span class=cl><span class=c1># 会认为第0行在 0*2=0，第1行在 1*2=2，...</span>
</span></span><span class=line><span class=cl><span class=c1># 但实际上第1行在内存偏移 1*1 = 1！</span>
</span></span></code></pre></td></tr></table></div></div><hr><h3 id=6-补充为什么-output-也需要-stride>6. 补充：为什么 output 也需要 stride？</h3><p>因为输出张量 <code>output = torch.empty_like(x)</code> 会<strong>继承输入的内存布局</strong>（包括 stride）。所以你也必须用 <code>output.stride(0)</code> 来正确写入结果。</p><hr><p>但其实我这个Softmax算子，更通用一点，也应该加入列的stride。</p><h2 id=mean算子>mean算子</h2><p>这个例子有点复杂</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span><span class=lnt>116
</span><span class=lnt>117
</span><span class=lnt>118
</span><span class=lnt>119
</span><span class=lnt>120
</span><span class=lnt>121
</span><span class=lnt>122
</span><span class=lnt>123
</span><span class=lnt>124
</span><span class=lnt>125
</span><span class=lnt>126
</span><span class=lnt>127
</span><span class=lnt>128
</span><span class=lnt>129
</span><span class=lnt>130
</span><span class=lnt>131
</span><span class=lnt>132
</span><span class=lnt>133
</span><span class=lnt>134
</span><span class=lnt>135
</span><span class=lnt>136
</span><span class=lnt>137
</span><span class=lnt>138
</span><span class=lnt>139
</span><span class=lnt>140
</span><span class=lnt>141
</span><span class=lnt>142
</span><span class=lnt>143
</span><span class=lnt>144
</span><span class=lnt>145
</span><span class=lnt>146
</span><span class=lnt>147
</span><span class=lnt>148
</span><span class=lnt>149
</span><span class=lnt>150
</span><span class=lnt>151
</span><span class=lnt>152
</span><span class=lnt>153
</span><span class=lnt>154
</span><span class=lnt>155
</span><span class=lnt>156
</span><span class=lnt>157
</span><span class=lnt>158
</span><span class=lnt>159
</span><span class=lnt>160
</span><span class=lnt>161
</span><span class=lnt>162
</span><span class=lnt>163
</span><span class=lnt>164
</span><span class=lnt>165
</span><span class=lnt>166
</span><span class=lnt>167
</span><span class=lnt>168
</span><span class=lnt>169
</span><span class=lnt>170
</span><span class=lnt>171
</span><span class=lnt>172
</span><span class=lnt>173
</span><span class=lnt>174
</span><span class=lnt>175
</span><span class=lnt>176
</span><span class=lnt>177
</span><span class=lnt>178
</span><span class=lnt>179
</span><span class=lnt>180
</span><span class=lnt>181
</span><span class=lnt>182
</span><span class=lnt>183
</span><span class=lnt>184
</span><span class=lnt>185
</span><span class=lnt>186
</span><span class=lnt>187
</span><span class=lnt>188
</span><span class=lnt>189
</span><span class=lnt>190
</span><span class=lnt>191
</span><span class=lnt>192
</span><span class=lnt>193
</span><span class=lnt>194
</span><span class=lnt>195
</span><span class=lnt>196
</span><span class=lnt>197
</span><span class=lnt>198
</span><span class=lnt>199
</span><span class=lnt>200
</span><span class=lnt>201
</span><span class=lnt>202
</span><span class=lnt>203
</span><span class=lnt>204
</span><span class=lnt>205
</span><span class=lnt>206
</span><span class=lnt>207
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=n>import</span> <span class=n>torch</span>
</span></span><span class=line><span class=cl><span class=n>import</span> <span class=n>triton</span>
</span></span><span class=line><span class=cl><span class=n>import</span> <span class=n>triton</span><span class=o>.</span><span class=n>language</span> <span class=n>as</span> <span class=n>tl</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=err>@</span><span class=n>triton</span><span class=o>.</span><span class=n>jit</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>mean_kernel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ptr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ptr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>input_stride0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>input_stride1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>input_stride2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>output_stride0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>output_stride1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>M</span><span class=p>,</span>  <span class=c1># size before reduction dim</span>
</span></span><span class=line><span class=cl>    <span class=n>N</span><span class=p>,</span>  <span class=c1># size of reduction dim</span>
</span></span><span class=line><span class=cl>    <span class=n>K</span><span class=p>,</span>  <span class=c1># size after reduction dim</span>
</span></span><span class=line><span class=cl>    <span class=n>BLOCK_SIZE</span><span class=p>:</span> <span class=n>tl</span><span class=o>.</span><span class=n>constexpr</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Kernel for computing mean along a single dimension.
</span></span></span><span class=line><span class=cl><span class=s2>    Input is viewed as (M, N, K) where N is the dimension being reduced.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># Program ID gives us which output element we&#39;re computing</span>
</span></span><span class=line><span class=cl>    <span class=n>pid</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>program_id</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Compute output indices</span>
</span></span><span class=line><span class=cl>    <span class=n>m_idx</span> <span class=o>=</span> <span class=n>pid</span> <span class=o>//</span> <span class=n>K</span>
</span></span><span class=line><span class=cl>    <span class=n>k_idx</span> <span class=o>=</span> <span class=n>pid</span> <span class=o>%</span> <span class=n>K</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Bounds check</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>m_idx</span> <span class=o>&gt;=</span> <span class=n>M</span> <span class=ow>or</span> <span class=n>k_idx</span> <span class=o>&gt;=</span> <span class=n>K</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Accumulate sum across reduction dimension</span>
</span></span><span class=line><span class=cl>    <span class=n>acc</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>n_start</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>n_offsets</span> <span class=o>=</span> <span class=n>n_start</span> <span class=o>+</span> <span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span> <span class=o>=</span> <span class=n>n_offsets</span> <span class=o>&lt;</span> <span class=n>N</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Calculate input indices</span>
</span></span><span class=line><span class=cl>        <span class=n>input_idx</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>m_idx</span> <span class=o>*</span> <span class=n>input_stride0</span> <span class=o>+</span> <span class=n>n_offsets</span> <span class=o>*</span> <span class=n>input_stride1</span> <span class=o>+</span> <span class=n>k_idx</span> <span class=o>*</span> <span class=n>input_stride2</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Load and accumulate</span>
</span></span><span class=line><span class=cl>        <span class=n>vals</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>input_ptr</span> <span class=o>+</span> <span class=n>input_idx</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span> <span class=n>other</span><span class=o>=</span><span class=mf>0.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>acc</span> <span class=o>+=</span> <span class=n>tl</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>vals</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Compute mean and store</span>
</span></span><span class=line><span class=cl>    <span class=n>mean_val</span> <span class=o>=</span> <span class=n>acc</span> <span class=o>/</span> <span class=n>N</span>
</span></span><span class=line><span class=cl>    <span class=n>output_idx</span> <span class=o>=</span> <span class=n>m_idx</span> <span class=o>*</span> <span class=n>output_stride0</span> <span class=o>+</span> <span class=n>k_idx</span> <span class=o>*</span> <span class=n>output_stride1</span>
</span></span><span class=line><span class=cl>    <span class=n>tl</span><span class=o>.</span><span class=n>store</span><span class=p>(</span><span class=n>output_ptr</span> <span class=o>+</span> <span class=n>output_idx</span><span class=p>,</span> <span class=n>mean_val</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>mean_dim</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>input</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>dim</span><span class=p>:</span> <span class=ne>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>keepdim</span><span class=p>:</span> <span class=ne>bool</span> <span class=o>=</span> <span class=n>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>dtype</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>dtype</span> <span class=o>|</span> <span class=n>None</span> <span class=o>=</span> <span class=n>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Triton implementation of torch.mean with single dimension reduction.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        input: Input tensor
</span></span></span><span class=line><span class=cl><span class=s2>        dim: Single dimension along which to compute mean
</span></span></span><span class=line><span class=cl><span class=s2>        keepdim: Whether to keep the reduced dimension
</span></span></span><span class=line><span class=cl><span class=s2>        dtype: Output dtype. If None, uses input dtype (or float32 for integer inputs)
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        Tensor with mean values along specified dimension
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># Validate inputs</span>
</span></span><span class=line><span class=cl>    <span class=nb>assert</span> <span class=n>input</span><span class=o>.</span><span class=n>is_cuda</span><span class=p>,</span> <span class=s2>&#34;Input must be a CUDA tensor&#34;</span>
</span></span><span class=line><span class=cl>    <span class=nb>assert</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=o>-</span><span class=n>input</span><span class=o>.</span><span class=n>ndim</span> <span class=o>&lt;=</span> <span class=n>dim</span> <span class=o>&lt;</span> <span class=n>input</span><span class=o>.</span><span class=n>ndim</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span> <span class=n>f</span><span class=s2>&#34;Invalid dimension {dim} for tensor with {input.ndim} dimensions&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Handle negative dim</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>dim</span> <span class=o>&lt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dim</span> <span class=o>=</span> <span class=n>dim</span> <span class=o>+</span> <span class=n>input</span><span class=o>.</span><span class=n>ndim</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Handle dtype</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>dtype</span> <span class=n>is</span> <span class=n>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>input</span><span class=o>.</span><span class=n>dtype</span> <span class=ow>in</span> <span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>int8</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>int16</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>int32</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>int64</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=n>dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>float32</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>dtype</span> <span class=o>=</span> <span class=n>input</span><span class=o>.</span><span class=n>dtype</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Convert input to appropriate dtype if needed</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>input</span><span class=o>.</span><span class=n>dtype</span> <span class=o>!=</span> <span class=n>dtype</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>input</span> <span class=o>=</span> <span class=n>input</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Get input shape and strides</span>
</span></span><span class=line><span class=cl>    <span class=n>shape</span> <span class=o>=</span> <span class=n>list</span><span class=p>(</span><span class=n>input</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Calculate dimensions for kernel</span>
</span></span><span class=line><span class=cl>    <span class=n>M</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>M</span> <span class=o>*=</span> <span class=n>shape</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>N</span> <span class=o>=</span> <span class=n>shape</span><span class=p>[</span><span class=n>dim</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>K</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>dim</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>len</span><span class=p>(</span><span class=n>shape</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span> <span class=o>*=</span> <span class=n>shape</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Reshape input to 3D view (M, N, K)</span>
</span></span><span class=line><span class=cl>    <span class=n>input_3d</span> <span class=o>=</span> <span class=n>input</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>M</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>K</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Create output shape</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>keepdim</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>output_shape</span> <span class=o>=</span> <span class=n>shape</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>output_shape</span><span class=p>[</span><span class=n>dim</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>output_shape</span> <span class=o>=</span> <span class=n>shape</span><span class=p>[:</span><span class=n>dim</span><span class=p>]</span> <span class=o>+</span> <span class=n>shape</span><span class=p>[</span><span class=n>dim</span> <span class=o>+</span> <span class=mi>1</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Create output tensor</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>empty</span><span class=p>(</span><span class=n>output_shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>input</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Reshape output for kernel</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>keepdim</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>output_2d</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>M</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>K</span><span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>output_2d</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>M</span><span class=p>,</span> <span class=n>K</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Launch kernel</span>
</span></span><span class=line><span class=cl>    <span class=n>grid</span> <span class=o>=</span> <span class=p>(</span><span class=n>M</span> <span class=o>*</span> <span class=n>K</span><span class=p>,)</span>
</span></span><span class=line><span class=cl>    <span class=n>BLOCK_SIZE</span> <span class=o>=</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>mean_kernel</span><span class=p>[</span><span class=n>grid</span><span class=p>](</span>
</span></span><span class=line><span class=cl>        <span class=n>input_3d</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>output_2d</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>input_3d</span><span class=o>.</span><span class=n>stride</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>input_3d</span><span class=o>.</span><span class=n>stride</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>input_3d</span><span class=o>.</span><span class=n>stride</span><span class=p>(</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>output_2d</span><span class=o>.</span><span class=n>stride</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>output_2d</span><span class=o>.</span><span class=n>stride</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=k>if</span> <span class=n>output_2d</span><span class=o>.</span><span class=n>ndim</span> <span class=o>&gt;</span> <span class=mi>1</span> <span class=k>else</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>M</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>N</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>BLOCK_SIZE</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>mean_batch_invariant</span><span class=p>(</span><span class=n>input</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=n>False</span><span class=p>,</span> <span class=n>dtype</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>dtype</span> <span class=o>|</span> <span class=n>None</span> <span class=o>=</span> <span class=n>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>assert</span> <span class=n>dtype</span> <span class=n>is</span> <span class=n>None</span> <span class=ow>or</span> <span class=n>dtype</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span> <span class=n>f</span><span class=s2>&#34;unsupported dtype: {dtype}&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>len</span><span class=p>(</span><span class=n>dim</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>mean_dim</span><span class=p>(</span><span class=n>input</span><span class=p>,</span> <span class=n>dim</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>keepdim</span><span class=o>=</span><span class=n>keepdim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>assert</span> <span class=n>input</span><span class=o>.</span><span class=n>dtype</span> <span class=ow>in</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span> <span class=s2>&#34;only float types supported for now&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>n_elems</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>dim</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>n_elems</span> <span class=o>*=</span> <span class=n>input</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=n>d</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>input</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=n>dim</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=n>keepdim</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span> <span class=o>/</span> <span class=n>n_elems</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>test_mean_batch_invariant</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=c1># 设置随机种子以确保结果可重复</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 定义测试的维度和大小</span>
</span></span><span class=line><span class=cl>    <span class=n>test_cases</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=c1># 一维张量</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=mi>1024</span><span class=p>,</span> <span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=c1># # 二维张量</span>
</span></span><span class=line><span class=cl>        <span class=c1># (512, 512),</span>
</span></span><span class=line><span class=cl>        <span class=c1># # 三维张量</span>
</span></span><span class=line><span class=cl>        <span class=c1># (16, 32, 2048),</span>
</span></span><span class=line><span class=cl>        <span class=c1># (2048, 4096, 64),</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 定义允许的误差范围</span>
</span></span><span class=line><span class=cl>    <span class=n>atol</span> <span class=o>=</span> <span class=mf>1e-3</span>
</span></span><span class=line><span class=cl>    <span class=n>rtol</span> <span class=o>=</span> <span class=mf>1e-3</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>shape</span> <span class=ow>in</span> <span class=n>test_cases</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 生成随机测试数据</span>
</span></span><span class=line><span class=cl>        <span class=n>input_tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=o>*</span><span class=n>shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;npu:0&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>input_dim</span> <span class=o>=</span> <span class=n>input_tensor</span><span class=o>.</span><span class=n>ndim</span>
</span></span><span class=line><span class=cl>        <span class=c1># print(f&#34;input = {input_tensor}, dim = {input_dim}&#34;)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 测试指定维度均值</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>dim</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>input_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=c1># 精度测试</span>
</span></span><span class=line><span class=cl>            <span class=n>output_torch</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>input_tensor</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=n>dim</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=n>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>output_triton</span> <span class=o>=</span> <span class=n>mean_batch_invariant</span><span class=p>(</span><span class=n>input_tensor</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=p>[</span><span class=n>dim</span><span class=p>],</span> <span class=n>keepdim</span><span class=o>=</span><span class=n>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=nb>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>allclose</span><span class=p>(</span><span class=n>output_torch</span><span class=p>,</span> <span class=n>output_triton</span><span class=p>,</span> <span class=n>atol</span><span class=o>=</span><span class=n>atol</span><span class=p>,</span> <span class=n>rtol</span><span class=o>=</span><span class=n>rtol</span><span class=p>),</span> \
</span></span><span class=line><span class=cl>                <span class=n>f</span><span class=s2>&#34;Test failed for shape {shape}. The difference between torch and triton on dim {dim} is {torch.abs(output_torch - output_triton)}.&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 性能测试</span>
</span></span><span class=line><span class=cl>            <span class=c1># t_torch = triton.testing.do_bench(lambda: torch.mean(input_tensor, dim=dim, keepdim=True))</span>
</span></span><span class=line><span class=cl>            <span class=c1># t_triton = triton.testing.do_bench(lambda: mean_batch_invariant(input_tensor, dim=[dim], keepdim=True))</span>
</span></span><span class=line><span class=cl>            <span class=c1># print(f&#34;Dim {dim}: runtime of torch = {t_torch} ms, runtime of triton = {t_triton} ms, times = {t_triton / t_torch}&#34;)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=n>f</span><span class=s2>&#34;Test passed for shape {shape}.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>test_mean_batch_invariant</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>在 PyTorch 中，<code>torch.reshape()</code> 的行为类似于 NumPy 的 <code>reshape</code>：<strong>它尽可能返回一个 view（视图）</strong>，但在某些情况下<strong>无法返回 view 时，会返回一个 copy（副本）</strong>。</p><h3 id=什么时候会返回-copy>什么时候会返回 copy？</h3><p>当张量的内存布局（由 <code>stride</code> 决定）不允许在不复制数据的情况下形成目标形状时，<code>reshape</code> 就必须返回一个 copy。</p><p>具体来说，如果原张量不是<strong>连续的（contiguous）</strong>，并且目标形状无法通过调整 stride 来表示，那么 <code>reshape</code> 就会触发数据复制。</p><hr><h3 id=举个例子>举个例子</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 创建一个非连续的张量</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>   <span class=c1># shape: (3, 4)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>t</span><span class=p>()</span>                            <span class=c1># 转置 -&gt; shape: (4, 3)，但内存不连续</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;y.is_contiguous():&#34;</span><span class=p>,</span> <span class=n>y</span><span class=o>.</span><span class=n>is_contiguous</span><span class=p>())</span>  <span class=c1># False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 尝试 reshape</span>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>y</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;z.is_contiguous():&#34;</span><span class=p>,</span> <span class=n>z</span><span class=o>.</span><span class=n>is_contiguous</span><span class=p>())</span>  <span class=c1># True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 检查是否是 view：修改 z 是否会影响 y？</span>
</span></span><span class=line><span class=cl><span class=c1># 注意：如果 z 是 y 的 view，那么修改 z 会影响 y（或底层数据）</span>
</span></span><span class=line><span class=cl><span class=c1># 但 reshape 返回的 z 是 copy，所以不会影响 y</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>z</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=o>-</span><span class=mi>999</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;y[0, 0]:&#34;</span><span class=p>,</span> <span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>])</span>  <span class=c1># 仍然是 0，说明 z 是 copy</span>
</span></span></code></pre></td></tr></table></div></div><p>输出类似：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>y.is_contiguous(): False
</span></span><span class=line><span class=cl>z.is_contiguous(): True
</span></span><span class=line><span class=cl>y[0, 0]: 0
</span></span></code></pre></td></tr></table></div></div><p>这说明：因为 <code>y</code> 是非连续的（由 <code>.t()</code> 产生），<code>y.reshape(12)</code> 无法只通过修改 stride 来展平，所以 PyTorch <strong>必须复制数据</strong>，返回一个新张量（copy）。</p><hr><h3 id=对比连续张量的-reshape返回-view>对比：连续张量的 reshape（返回 view）</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span>          <span class=c1># contiguous</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>a</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>           <span class=c1># view</span>
</span></span><span class=line><span class=cl><span class=n>b</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=mi>888</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>a</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>  <span class=c1># 输出 888，说明是 view</span>
</span></span></code></pre></td></tr></table></div></div><hr><h3 id=总结>总结</h3><ul><li><code>torch.reshape()</code> <strong>优先返回 view</strong>；</li><li><strong>当输入张量不连续且目标形状无法通过 stride 表达时，会返回 copy</strong>；</li><li>判断是否是 view 的一个实用方式：检查 <code>.is_contiguous()</code> 或通过修改元素看是否影响原张量（但后者不推荐用于生产代码）；</li><li>如果你<strong>明确需要 view</strong>，可使用 <code>.view()</code>：它在无法创建 view 时会报错；</li><li>如果你<strong>明确需要新内存</strong>，可以用 <code>.reshape().clone()</code> 或先 <code>.contiguous()</code> 再 <code>.view()</code>。</li></ul><h2 id=自动调优autotune>自动调优autotune</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton.language</span> <span class=k>as</span> <span class=nn>tl</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 定义 autotune 的配置</span>
</span></span><span class=line><span class=cl><span class=nd>@triton.autotune</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>configs</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>triton</span><span class=o>.</span><span class=n>Config</span><span class=p>({</span><span class=s1>&#39;BLOCK_SIZE&#39;</span><span class=p>:</span> <span class=mi>128</span><span class=p>}),</span>
</span></span><span class=line><span class=cl>        <span class=n>triton</span><span class=o>.</span><span class=n>Config</span><span class=p>({</span><span class=s1>&#39;BLOCK_SIZE&#39;</span><span class=p>:</span> <span class=mi>256</span><span class=p>}),</span>
</span></span><span class=line><span class=cl>        <span class=n>triton</span><span class=o>.</span><span class=n>Config</span><span class=p>({</span><span class=s1>&#39;BLOCK_SIZE&#39;</span><span class=p>:</span> <span class=mi>512</span><span class=p>}),</span>
</span></span><span class=line><span class=cl>        <span class=n>triton</span><span class=o>.</span><span class=n>Config</span><span class=p>({</span><span class=s1>&#39;BLOCK_SIZE&#39;</span><span class=p>:</span> <span class=mi>1024</span><span class=p>}),</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>key</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;n&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nd>@triton.jit</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>add_kernel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>x_ptr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>y_ptr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ptr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>n</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>BLOCK_SIZE</span><span class=p>:</span> <span class=n>tl</span><span class=o>.</span><span class=n>constexpr</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>pid</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>program_id</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>offsets</span> <span class=o>=</span> <span class=n>pid</span> <span class=o>*</span> <span class=n>BLOCK_SIZE</span> <span class=o>+</span> <span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>offsets</span> <span class=o>&lt;</span> <span class=n>n</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>x_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>y_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>    <span class=n>tl</span><span class=o>.</span><span class=n>store</span><span class=p>(</span><span class=n>output_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>output</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>add</span><span class=p>(</span><span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>y</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>empty_like</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>n</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>grid</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>meta</span><span class=p>:</span> <span class=p>(</span><span class=n>triton</span><span class=o>.</span><span class=n>cdiv</span><span class=p>(</span><span class=n>n</span><span class=p>,</span> <span class=n>meta</span><span class=p>[</span><span class=s1>&#39;BLOCK_SIZE&#39;</span><span class=p>]),)</span>
</span></span><span class=line><span class=cl>    <span class=n>add_kernel</span><span class=p>[</span><span class=n>grid</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>output</span><span class=p>,</span> <span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>size</span> <span class=o>=</span> <span class=mi>98431</span>  <span class=c1># 非2的幂，更能体现 autotune 的作用</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;npu:0&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;npu:0&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>add</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>npu</span><span class=o>.</span><span class=n>synchronize</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>end</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>[:</span><span class=mi>5</span><span class=p>])</span>  <span class=c1># 打印前5个元素验证正确性</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Kernel time: </span><span class=si>{</span><span class=p>(</span><span class=n>end</span> <span class=o>-</span> <span class=n>start</span><span class=p>)</span> <span class=o>*</span> <span class=mi>1000</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2> ms&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2025-12-18&nbsp;<a class=git-hash href=https://github.com/pigLoveRabbit520/commit/9a69cad358a9bdabd089e5de8e2e3a8049c2d331 target=_blank title="commit by pigLoveRabbit(pigLoveRabbit520@outlook.com) 9a69cad358a9bdabd089e5de8e2e3a8049c2d331: Add autotuning example in Triton_learning.md">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>9a69cad</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/triton_learning/index.md target=_blank>阅读原始文档</a></span></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/torch_learning/ class=prev rel=prev title=pytorch学习哈><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>pytorch学习哈</a>
<a href=/triton_attention/ class=next rel=next title=Triton写Attention算子>Triton写Attention算子<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.134.2">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2019 - 2026</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/rabbitLove520 target=_blank>pigLoveRabbit</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:50},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{algoliaAppID:"PASDMWALPK",algoliaIndex:"index.zh-cn",algoliaSearchKey:"b42948e51daaa93df92381c8e2ac0f93",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>猪爱兔的网站</title><link>https://rabbitLove520.github.io/</link><description>Javascript NodeJs C# software developer</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 18 Jan 2026 21:00:00 +0000</lastBuildDate><atom:link href="https://rabbitLove520.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Verilog学习</title><link>https://rabbitLove520.github.io/verilog_learning/</link><pubDate>Sun, 18 Jan 2026 21:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://rabbitLove520.github.io/verilog_learning/</guid><description><![CDATA[<p></p>
<!-- more -->
<h2 id="基本语法">基本语法</h2>
<p>Verilog HDL（简称 Verilog ）是一种硬件描述语言，用于数字电路的系统设计。可对算法级、门级、开关级等多种抽象设计层次进行建模。<br>
Verilog 继承了 C 语言的多种操作符和结构，与另一种硬件描述语言 VHDL 相比，语法不是很严格，代码更加简洁，更容易上手。<br>
Verilog 不仅定义了语法，还对语法结构都定义了清晰的仿真语义。因此，Verilog 编写的数字模型就能够使用 Verilog 仿真器进行验证。
当然可以！以下是 <strong>Verilog HDL（硬件描述语言）的基础语法总结</strong>，适合初学者快速掌握核心概念，也方便你后续进行数字电路设计、仿真和综合。</p>]]></description></item><item><title>Triton写Attention算子</title><link>https://rabbitLove520.github.io/triton_attention/</link><pubDate>Sun, 26 Oct 2025 14:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://rabbitLove520.github.io/triton_attention/</guid><description><![CDATA[<p></p>
<h2 id="triton">Triton</h2>
<p><a href="https://triton.hyper.ai/" target="_blank" rel="noopener noreffer ">官网介绍</a>：Triton 是一种用于并行编程的语言和编译器。它旨在提供一个基于 Python 的编程环境，以高效编写自定义 DNN 计算内核，并能够在现代 GPU 硬件上以最大吞吐量运行。<br>
简单讲，就是可以用Python写GPU算子。</p>]]></description></item><item><title>Triton写简单算子</title><link>https://rabbitLove520.github.io/triton_learning/</link><pubDate>Thu, 02 Oct 2025 14:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://rabbitLove520.github.io/triton_learning/</guid><description><![CDATA[<p></p>
<h2 id="triton">Triton</h2>
<p><a href="https://triton.hyper.ai/" target="_blank" rel="noopener noreffer ">官网介绍</a>：Triton 是一种用于并行编程的语言和编译器。它旨在提供一个基于 Python 的编程环境，以高效编写自定义 DNN 计算内核，并能够在现代 GPU 硬件上以最大吞吐量运行。<br>
简单讲，就是可以用Python写GPU算子。</p>]]></description></item><item><title>pytorch学习哈</title><link>https://rabbitLove520.github.io/torch_learning/</link><pubDate>Mon, 30 Dec 2024 22:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://rabbitLove520.github.io/torch_learning/</guid><description><![CDATA[<p></p>
<!-- more -->
<h2 id="backward函数">backward函数</h2>
<p><code>backward()</code> 是深度学习框架（如 <strong>PyTorch</strong>）中的一个核心方法，它的作用是：<strong>自动计算损失函数对模型参数的梯度（gradient）</strong>，为后续的参数更新（比如用 SGD、Adam 等优化器）提供依据。</p>]]></description></item><item><title>Attention机制学习</title><link>https://rabbitLove520.github.io/llm_learning/</link><pubDate>Sun, 01 Dec 2024 20:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://rabbitLove520.github.io/llm_learning/</guid><description><![CDATA[<p></p>
<!-- more -->
<h2 id="q-k-v">Q K V</h2>
<p>在深度学习中，很多 LLM 的训练都使用 Transformer 架构，而在 Transformer 架构中计算的过程涉及到的最关键的就是注意力，它是整个过程中重要的基础。注意力抽象出了 3 个重要的概念，在计算过程中对应着 3 个矩阵，如下所示：</p>]]></description></item><item><title>Ascend C学习</title><link>https://rabbitLove520.github.io/ascend_c/</link><pubDate>Fri, 08 Nov 2024 19:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://rabbitLove520.github.io/ascend_c/</guid><description><![CDATA[<p></p>
<!-- more -->
<h2 id="b站视频">B站视频</h2>
<p><a href="https://www.bilibili.com/video/BV1bz421q7tg" target="_blank" rel="noopener noreffer ">Ascend C算子开发中级教程</a>
Tiling过程可以在Host侧完成用于动态shape。</p>]]></description></item><item><title>pytorch手写数字识别</title><link>https://rabbitLove520.github.io/my_pytorch/</link><pubDate>Tue, 29 Oct 2024 20:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://rabbitLove520.github.io/my_pytorch/</guid><description><![CDATA[<p></p>
<!-- more -->
<h2 id="一个例子">一个例子</h2>
<p>csv在这里<a href="https://github.com/npradaschnor/Pima-Indians-Diabetes-Dataset/blob/master/diabetes.csv" target="_blank" rel="noopener noreffer ">下载</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torchvision</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 数据预处理</span>
</span></span><span class="line"><span class="cl"><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,))</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 加载数据集</span>
</span></span><span class="line"><span class="cl"><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;data/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;data/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 定义神经网络模型</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 实例化模型</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 定义损失函数和优化器</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 训练模型</span>
</span></span><span class="line"><span class="cl"><span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在测试集上评估模型</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>以上代码分为几个过程</p>]]></description></item><item><title>Keras 简单尝试</title><link>https://rabbitLove520.github.io/keras_first/</link><pubDate>Fri, 25 Oct 2024 19:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://rabbitLove520.github.io/keras_first/</guid><description><![CDATA[<p></p>
<!-- more -->
<h2 id="安装keras">安装Keras</h2>
<p>Keras 2 是一个较旧的版本（最新主版本为 Keras 3，但许多用户仍在使用 Keras 2.x，尤其是配合 TensorFlow 1.x 或 2.x 早期版本）。<br>
装TensorFlow 2.15的时候，Keras 2自动就带上了。</p>]]></description></item><item><title>最小二乘法和线性回归</title><link>https://rabbitLove520.github.io/least_square/</link><pubDate>Tue, 08 Oct 2024 14:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://rabbitLove520.github.io/least_square/</guid><description><![CDATA[<h2 id="最小二乘法">最小二乘法</h2>
<p>早在19世纪,勒让德就认为让&quot;误差的平方和最小&quot;估计出来的模型是最接近真实情形的。<br>
按照勒让德的最佳原则,于是就是求:<br>
$$
\text{L} = \sum_{i=1}^{n} \left( y_i - f(x_i) \right)^2
$$
这个目标函数取得最小值时的函数参数,这就是最小二乘法的思想想,所谓&quot;二乘&quot;就是平方的意思。从这里我们可以看到,<strong>最小二乘法其实
就是用来做函数拟合的一种思想</strong>。<br>
至于怎么求出具体的参数那就是另外一个问题了,理论上可以用导数法、几何法,工程上可以用<strong>梯度下降法</strong>。下面以最常用的线性回归为
例进行推导和理解。<br>
在<strong>机器学习</strong>中用于回归问题的损失函数(Loss Function)是均方误差(MSE)：
$$
\text{L} = \frac{1}{2n} \sum_{i=1}^{n} \left( y_i - f(x_i) \right)^2
$$
其实就是多了个1/2n。</p>]]></description></item><item><title>JavaScript中的微任务、宏任务和Promise</title><link>https://rabbitLove520.github.io/javascript_%E4%BB%BB%E5%8A%A1%E9%98%9F%E5%88%97_promise/</link><pubDate>Wed, 18 Sep 2024 09:00:00 +0000</pubDate><author>pigLoveRabbit</author><guid>https://rabbitLove520.github.io/javascript_%E4%BB%BB%E5%8A%A1%E9%98%9F%E5%88%97_promise/</guid><description>&lt;p>JavaScript会将异步任务划分为微任务和宏任务，微任务会在宏任务之前执行（因为每次从主线程切换到任务队列时，都会优先遍历微任务队列，后遍历宏任务队列）。&lt;/p></description></item></channel></rss>
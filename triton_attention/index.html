<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Triton写Attention算子 - 猪爱兔的网站</title><meta name=Description content="Javascript NodeJs C# software developer"><meta property="og:url" content="https://rabbitLove520.github.io/triton_attention/">
<meta property="og:site_name" content="猪爱兔的网站"><meta property="og:title" content="Triton写Attention算子"><meta property="og:description" content="
Triton 官网介绍：Triton 是一种用于并行编程的语言和编译器。它旨在提供一个基于 Python 的编程环境，以高效编写自定义 DNN 计算内核，并能够在现代 GPU 硬件上以最大吞吐量运行。
简单讲，就是可以用Python写GPU算子。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-26T14:00:00+00:00"><meta property="article:modified_time" content="2026-01-31T15:32:12+08:00"><meta property="og:image" content="https://rabbitLove520.github.io/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rabbitLove520.github.io/logo.png"><meta name=twitter:title content="Triton写Attention算子"><meta name=twitter:description content="
Triton 官网介绍：Triton 是一种用于并行编程的语言和编译器。它旨在提供一个基于 Python 的编程环境，以高效编写自定义 DNN 计算内核，并能够在现代 GPU 硬件上以最大吞吐量运行。
简单讲，就是可以用Python写GPU算子。"><meta name=application-name content="pigLoveRabbit"><meta name=apple-mobile-web-app-title content="pigLoveRabbit"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://rabbitLove520.github.io/triton_attention/><link rel=prev href=https://rabbitLove520.github.io/triton_learning/><link rel=next href=https://rabbitLove520.github.io/verilog_learning/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Triton写Attention算子","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/rabbitLove520.github.io\/triton_attention\/"},"image":["https:\/\/rabbitLove520.github.io\/images\/Apple-Devices-Preview.png"],"genre":"posts","wordcount":4673,"url":"https:\/\/rabbitLove520.github.io\/triton_attention\/","datePublished":"2025-10-26T14:00:00+00:00","dateModified":"2026-01-31T15:32:12+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"xxxx","logo":"https:\/\/rabbitLove520.github.io\/images\/avatar.jpg"},"author":{"@type":"Person","name":"pigLoveRabbit"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=猪爱兔的网站><span class=header-title-pre><i class='far fa-kiss-wink-heart fa-fw' aria-hidden=true></i></span>pigLoveRabbit</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>所有文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/categories/documentation/>文档 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://github.com/pigLoveRabbit520 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="menu-item language" title=选择语言><i class="fa fa-globe" aria-hidden=true></i>
<select class=language-select id=language-select-desktop onchange="location=this.value"><option value=/triton_attention/ selected>简体中文</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=猪爱兔的网站><span class=header-title-pre><i class='far fa-kiss-wink-heart fa-fw' aria-hidden=true></i></span>pigLoveRabbit</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/posts/ title>所有文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/categories/documentation/ title>文档</a><a class=menu-item href=/about/ title>关于</a><a class=menu-item href=https://github.com/pigLoveRabbit520 title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class=menu-item title=选择语言><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select onchange="location=this.value"><option value=/triton_attention/ selected>简体中文</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Triton写Attention算子</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/rabbitLove520 title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>pigLoveRabbit</a></span>&nbsp;<span class=post-category>收录于 <a href=/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/><i class="far fa-folder fa-fw" aria-hidden=true></i>深度学习</a>&nbsp;<a href=/categories/python/><i class="far fa-folder fa-fw" aria-hidden=true></i>Python</a>&nbsp;<a href=/categories/triton/><i class="far fa-folder fa-fw" aria-hidden=true></i>Triton</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2025-10-26>2025-10-26</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;约 4673 字&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;预计阅读 10 分钟&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#triton>Triton</a></li><li><a href=#bnsd>BNSD</a><ul><li><a href=#多头注意力multi-head-attention>多头注意力（Multi-Head Attention）</a></li><li><a href=#详细解释>详细解释：</a></li><li><a href=#示例bert-base>示例（BERT-base）</a></li><li><a href=#为什么是-bnsd>为什么是 BNSD？</a></li></ul></li><li><a href=#最简单的attention算子>最简单的Attention算子</a><ul><li><a href=#关键部分解释>关键部分解释</a></li></ul></li><li><a href=#transfomer模型>Transfomer模型</a><ul><li><a href=#1-训练阶段的-outputs-shifted-right-是什么为什么需要它>1. 训练阶段的 “Outputs (shifted right)” 是什么？为什么需要它？</a></li><li><a href=#2-推理阶段的输入是什么>2. 推理阶段的输入是什么？</a></li><li><a href=#总结对比>总结对比</a></li><li><a href=#ffn层>FFN层：</a></li><li><a href=#为什么说它们是全连接>为什么说它们是“全连接”？</a><ul><li><a href=#补充说明>补充说明：</a></li></ul></li><li><a href=#总结>总结：</a></li><li><a href=#add--norm-一句话解释>Add & Norm 一句话解释：</a></li><li><a href=#-详细拆解>🔍 详细拆解：</a><ul><li><a href=#1-add残差连接--residual-connection>1. <strong>Add（残差连接 / Residual Connection）</strong></a></li><li><a href=#2-normlayer-normalization>2. <strong>Norm（Layer Normalization）</strong></a></li></ul></li><li><a href=#-在-transformer-中的具体位置>📦 在 Transformer 中的具体位置：</a><ul><li><a href=#encoder-层结构重复-n-次>Encoder 层结构（重复 N 次）：</a></li><li><a href=#decoder-层结构重复-n-次>Decoder 层结构（重复 N 次）：</a></li></ul></li><li><a href=#-举个简单例子数值示意>🧠 举个简单例子（数值示意）：</a></li><li><a href=#为什么叫-add--norm-而不是-norm--add>❓为什么叫 “Add & Norm” 而不是 “Norm & Add”？</a></li><li><a href=#-总结>✅ 总结：</a></li></ul></li><li><a href=#mlpmulti-layer-perceptron多层感知机>MLP（Multi-Layer Perceptron，多层感知机）</a><ul><li><a href=#1-mlp多层感知机>1. <strong>MLP（多层感知机）</strong></a></li><li><a href=#2-ffn前馈网络>2. <strong>FFN（前馈网络）</strong></a></li><li><a href=#主要区别总结>主要区别总结：</a></li></ul></li><li><a href=#参考>参考</a></li></ul></nav></div></div><div class=content id=content><p><img class=lazyload src=/svg/loading.min.svg data-src=/images/triton-logo.png data-srcset="/images/triton-logo.png, /images/triton-logo.png 1.5x, /images/triton-logo.png 2x" data-sizes=auto alt=/images/triton-logo.png title="upload successful"></p><h2 id=triton>Triton</h2><p><a href=https://triton.hyper.ai/ target=_blank rel="noopener noreffer">官网介绍</a>：Triton 是一种用于并行编程的语言和编译器。它旨在提供一个基于 Python 的编程环境，以高效编写自定义 DNN 计算内核，并能够在现代 GPU 硬件上以最大吞吐量运行。<br>简单讲，就是可以用Python写GPU算子。</p><h2 id=bnsd>BNSD</h2><p>普通的Attention是单头注意力：<br>输入形状：[batch_size, seq_len, d_model]</p><ul><li>Q: <code>[batch_size, seq_len, d_k]</code></li><li>K: <code>[batch_size, seq_len, d_k]</code></li><li>V: <code>[batch_size, seq_len, d_v]</code></li></ul><p>在标准 Transformer 中通常设 d_k = d_v = d_model。</p><p>但更常用是多头注意力（Multi-Head Attention）。</p><h3 id=多头注意力multi-head-attention>多头注意力（Multi-Head Attention）</h3><p>在深度学习特别是Transformer架构中的Attention机制里，<strong>query、key、value</strong> 的维度通常被描述为 <strong>(B, N, S, D)</strong> 或类似的格式，其中：</p><hr><p>✅ <strong>B</strong> = Batch size<br>✅ <strong>S</strong> = Sequence length（序列长度）<br>✅ <strong>N</strong> = Number of attention heads（注意力头数）<br>✅ <strong>D</strong> = Dimension per head（每个头的维度）</p><hr><h3 id=详细解释>详细解释：</h3><ol><li><p><strong>B (Batch Size)</strong><br>表示一次前向传播中处理的样本数量。比如一次处理32个句子，B=32。</p></li><li><p><strong>S (Sequence Length)</strong><br>表示每个样本中的 token 数量。比如一个句子有50个词，S=50。</p></li><li><p><strong>N (Number of Heads)</strong><br>在多头注意力（Multi-Head Attention）中，query/key/value 会被拆分成 N 个“头”，每个头独立计算注意力。比如 N=8。</p></li><li><p><strong>D (Dimension per Head)</strong><br>每个注意力头的向量维度。原始的 embedding 维度（如512）会被平均分配到每个头上。例如：总维度512，8个头 → D = 512 / 8 = 64。</p></li></ol><hr><h3 id=示例bert-base>示例（BERT-base）</h3><p>假设：</p><ul><li>batch_size = 32</li><li>seq_len = 128</li><li>d_model = 768</li><li>num_heads = 12</li><li>head_dim = 768 // 12 = 64</li></ul><p>那么 query 的形状就是：<br>👉 <code>[32, 12, 128, 64]</code> —— 即 <strong>BNSD</strong></p><hr><h3 id=为什么是-bnsd>为什么是 BNSD？</h3><p>BNSD（[Batch, Num_heads, Seq_len, Head_dim]）成为主流布局的核心原因在于 它与注意力计算的数学本质和 GPU 硬件执行模型高度匹配。</p><hr><h2 id=最简单的attention算子>最简单的Attention算子</h2><p>Attention公式
$$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^\top}{\sqrt{d_k}} \right) V
$$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>simple_attention</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    最简单的 scaled dot-product attention
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        query: (..., seq_len_q, d_k)
</span></span></span><span class=line><span class=cl><span class=s2>        key:   (..., seq_len_k, d_k)
</span></span></span><span class=line><span class=cl><span class=s2>        value: (..., seq_len_k, d_v)
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        output: (..., seq_len_q, d_v)
</span></span></span><span class=line><span class=cl><span class=s2>        attention_weights: (..., seq_len_q, seq_len_k)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>d_k</span> <span class=o>=</span> <span class=n>query</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>d_k</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>attention_weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attention_weights</span><span class=p>,</span> <span class=n>value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>output</span><span class=p>,</span> <span class=n>attention_weights</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例用法</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># 假设 batch_size=2, seq_len=3, d_k=d_v=4</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_k</span><span class=p>,</span> <span class=n>d_v</span> <span class=o>=</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl>    <span class=n>Q</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>K</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>V</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_v</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>output</span><span class=p>,</span> <span class=n>weights</span> <span class=o>=</span> <span class=n>simple_attention</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Output shape:&#34;</span><span class=p>,</span> <span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>        <span class=c1># [2, 3, 4]</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Weights shape:&#34;</span><span class=p>,</span> <span class=n>weights</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>      <span class=c1># [2, 3, 3]</span>
</span></span></code></pre></td></tr></table></div></div><p>这个实现没有 mask、没有多头、没有线性变换，是最核心、最简化的 attention 逻辑。适合学习理解。</p><h3 id=关键部分解释>关键部分解释</h3><ol><li><code>query</code>、<code>key</code>、<code>value</code> 是标准的注意力三元组。</li><li>计算 query 和 key 的点积（相似度）。</li><li>除以 sqrt(d_k) 进行缩放（防止点积过大导致 softmax 梯度消失）。</li><li>对结果做 softmax 得到注意力权重。</li><li>用权重对 value 加权求和，得到输出。</li></ol><h2 id=transfomer模型>Transfomer模型</h2><p><img class=lazyload src=/svg/loading.min.svg data-src=https://transformers.run/assets/img/attention/transformer.jpeg data-srcset="https://transformers.run/assets/img/attention/transformer.jpeg, https://transformers.run/assets/img/attention/transformer.jpeg 1.5x, https://transformers.run/assets/img/attention/transformer.jpeg 2x" data-sizes=auto alt=https://transformers.run/assets/img/attention/transformer.jpeg title=transformer>
左边是Encoder，右边是Decoder</p><p><strong>在推理阶段，没有“Outputs (shifted right)”这个明确的输入。</strong> 这个概念是训练阶段为了模拟推理行为而设计的一种“技巧”或“教学工具”。</p><p>我们来详细解释一下：</p><hr><h3 id=1-训练阶段的-outputs-shifted-right-是什么为什么需要它>1. 训练阶段的 “Outputs (shifted right)” 是什么？为什么需要它？</h3><p>在训练时，我们为了并行高效，会一次性把整个目标序列（比如 “我 爱 你 <eos>”）喂给Decoder。</p><ul><li><p><strong>问题：</strong> 如果直接输入 “我 爱 你 <eos>”，在预测第一个词 “我” 的时候，模型就会“偷看”到后面的 “爱”、“你”，这违反了“根据上文预测下一个词”的自回归法则。这被称为<strong>信息泄露</strong>。</p></li><li><p><strong>解决方案：</strong> “Shifted Right”（右移）</p><ul><li><strong>Decoder的输入：</strong> 我们把目标序列的<strong>开始符 <code>&lt;sos></code></strong> 放在最前面，并<strong>去掉最后一个词</strong>，变成：<code>&lt;sos> 我 爱</code></li><li><strong>训练标签/目标：</strong> 就是原始的目标序列，但<strong>去掉开始符</strong>，变成：<code>我 爱 你 &lt;eos></code></li></ul></li></ul><p>这样，当Decoder的输入是 <code>&lt;sos> 我 爱</code> 时，它被要求并行地输出 <code>我 爱 你 &lt;eos></code>。我们来看一下这个对应关系：</p><table><thead><tr><th style=text-align:left>输入 (Shifted Right)</th><th style=text-align:left>→</th><th style=text-align:left>模型被要求预测的输出</th></tr></thead><tbody><tr><td style=text-align:left><code>&lt;sos></code></td><td style=text-align:left>→</td><td style=text-align:left><strong>我</strong></td></tr><tr><td style=text-align:left><code>&lt;sos></code> <strong>我</strong></td><td style=text-align:left>→</td><td style=text-align:left><strong>爱</strong></td></tr><tr><td style=text-align:left><code>&lt;sos></code> 我 <strong>爱</strong></td><td style=text-align:left>→</td><td style=text-align:left><strong>你</strong></td></tr><tr><td style=text-align:left><code>&lt;sos></code> 我 爱 <strong>你</strong></td><td style=text-align:left>→</td><td style=text-align:left><strong><code>&lt;eos></code></strong></td></tr></tbody></table><p>通过<strong>掩码自注意力</strong>，确保在计算每个位置的输出时，只能看到它自己和它左边的输入。这样，对于“预测‘爱’”这个任务，模型的输入能看到 <code>&lt;sos></code> 和 “我”，但看不到后面的“爱”和“你”，完美模拟了推理时的情况。</p><p><strong>所以，“Outputs (shifted right)”是训练时为了并行化而构造的一个“伪输入”。</strong></p><hr><h3 id=2-推理阶段的输入是什么>2. 推理阶段的输入是什么？</h3><p>推理时，我们<strong>没有</strong>完整的目标序列，因此无法构造一个“shifted right”的版本。推理的输入是<strong>动态生成</strong>的。</p><p><strong>推理输入 = 起始符 + 模型自己历史生成的全部令牌</strong></p><p>这个过程是循环进行的：</p><ol><li><strong>初始输入：</strong> <code>[&lt;sos>]</code></li><li><strong>第一轮输出：</strong> 模型接收 <code>[&lt;sos>]</code>，输出第一个词 <code>我</code>。</li><li><strong>更新输入：</strong> 将 <code>我</code> 追加到输入中，得到 <code>[&lt;sos>, 我]</code></li><li><strong>第二轮输出：</strong> 模型接收 <code>[&lt;sos>, 我]</code>，输出下一个词 <code>爱</code>。</li><li><strong>更新输入：</strong> 将 <code>爱</code> 追加到输入中，得到 <code>[&lt;sos>, 我, 爱]</code></li><li>&mldr; 如此循环，直到生成 <code>&lt;eos></code>。</li></ol><h3 id=总结对比>总结对比</h3><table><thead><tr><th style=text-align:left>阶段</th><th style=text-align:left>输入来源</th><th style=text-align:left>“Outputs (shifted right)”？</th></tr></thead><tbody><tr><td style=text-align:left><strong>训练</strong></td><td style=text-align:left>来自<strong>训练数据集</strong>中准备好的完整目标序列。</td><td style=text-align:left><strong>有</strong>。这是一个为了并行训练而特意构造的输入。</td></tr><tr><td style=text-align:left><strong>推理</strong></td><td style=text-align:left>来自<strong>模型自身前一步的输出</strong>，动态拼接而成。</td><td style=text-align:left><strong>没有</strong>。因为根本没有现成的“Outputs”可供“Shift”。</td></tr></tbody></table><p><strong>一个形象的比喻：</strong></p><ul><li><strong>训练：</strong> 像老师在教学生完形填空。老师直接把一整段文字（shifted right）给学生看，但用纸盖住后面的部分（掩码），让学生同时填写所有的空。</li><li><strong>推理：</strong> 像学生自己写作文。他只能从第一个字开始写，写下的每一个字都成为下文的基础。他不可能提前拿到一篇“右移”的作文。</li></ul><p>因此，<strong>“Outputs (shifted right)”是训练阶段独有的一个数据准备步骤，在推理阶段不存在。</strong> 推理阶段的输入是自回归地、逐步构建起来的。</p><p>##是的，<strong>Transformer 中的 FFN（Feed-Forward Network，前馈神经网络）层本质上是由两个全连接层（Fully Connected Layers）组成的</strong>，中间夹着一个非线性激活函数。</p><h3 id=ffn层>FFN层：</h3><p>在原始的 Transformer 论文（《Attention is All You Need》）中，FFN 层的结构如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>FFN(x) = W₂ * GELU(W₁ * x + b₁) + b₂
</span></span></code></pre></td></tr></table></div></div><p>或者在早期版本中使用的是 ReLU：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>FFN(x) = W₂ * ReLU(W₁ * x + b₁) + b₂
</span></span></code></pre></td></tr></table></div></div><p>其中：</p><ul><li><code>W₁</code> 和 <code>W₂</code> 是可学习的权重矩阵（即全连接层的参数），</li><li><code>b₁</code> 和 <code>b₂</code> 是偏置项，</li><li><code>GELU</code> 或 <code>ReLU</code> 是非线性激活函数。</li></ul><h3 id=为什么说它们是全连接>为什么说它们是“全连接”？</h3><ul><li><strong>第一层</strong>：将输入向量（维度为 <code>d_model</code>）线性变换到一个更高维的隐藏空间（维度通常为 <code>4 * d_model</code>），这是一个标准的线性变换 → <strong>全连接层</strong>。</li><li><strong>激活函数</strong>：引入非线性（如 ReLU、GELU）。</li><li><strong>第二层</strong>：将隐藏层向量再线性变换回原始维度 <code>d_model</code> → <strong>另一个全连接层</strong>。</li></ul><p>这两个线性变换都没有使用卷积、循环结构或注意力机制，纯粹是矩阵乘法 + 偏置，因此<strong>完全符合“全连接层”的定义</strong>。</p><h4 id=补充说明>补充说明：</h4><ul><li><strong>每个位置独立处理</strong>：FFN 对序列中每个位置的表示独立应用相同的网络结构（即权重共享），这与自注意力机制不同（自注意力是跨位置交互的）。</li><li><strong>参数量大</strong>：由于隐藏层维度通常是 <code>d_model</code> 的 4 倍，FFN 是 Transformer 中参数量最大的部分之一（通常超过注意力机制）。</li><li><strong>变体存在</strong>：虽然标准 FFN 是两个全连接层，但后续研究也探索了其他结构（如使用卷积、稀疏连接、MoE 等），但在原始 Transformer 和绝大多数实现中，<strong>FFN 就是两个全连接层</strong>。</li></ul><h3 id=总结>总结：</h3><p>✅ <strong>是的，Transformer 中的 FFN 层由两个全连接层构成</strong>，中间加一个非线性激活函数。这是其标准和最广泛使用的结构。</p><p>在你提到的 Transformer 架构图（例如 <a href=https://transformers.run/assets/img/attention/transformer.jpeg target=_blank rel="noopener noreffer">https://transformers.run/assets/img/attention/transformer.jpeg</a>）中，<strong>“Add & Norm”</strong> 是 <strong>“Add and Layer Normalization”</strong> 的缩写，它是 Transformer 模型中一个<strong>关键的子层结构</strong>，用于稳定训练、加速收敛并提升模型性能。</p><hr><h3 id=add--norm-一句话解释>Add & Norm 一句话解释：</h3><blockquote><p><strong>“Add & Norm” = 残差连接（Add） + 层归一化（Layer Normalization）</strong></p></blockquote><p>它出现在 <strong>Encoder 和 Decoder 的每个子模块之后</strong>，具体有两个位置：</p><ol><li><strong>自注意力（或 cross-attention）之后</strong></li><li><strong>前馈神经网络（Feed Forward）之后</strong></li></ol><hr><h3 id=-详细拆解>🔍 详细拆解：</h3><h4 id=1-add残差连接--residual-connection>1. <strong>Add（残差连接 / Residual Connection）</strong></h4><ul><li>将<strong>子层的输入</strong>和<strong>子层的输出</strong>相加：<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>sublayer</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span> <span class=o>+</span> <span class=nb>input</span>
</span></span></code></pre></td></tr></table></div></div></li><li>作用：<ul><li>缓解深层网络中的梯度消失问题；</li><li>允许信息直接“跳过”某些层，保留原始特征；</li><li>是 ResNet 思想在 Transformer 中的应用。</li></ul></li></ul><h4 id=2-normlayer-normalization>2. <strong>Norm（Layer Normalization）</strong></h4><ul><li>对 <strong>Add 之后的结果</strong>做层归一化：<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>normalized_output</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>sublayer</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span> <span class=o>+</span> <span class=nb>input</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></li><li>LayerNorm 是对<strong>单个样本的所有特征维度</strong>做归一化（与 BatchNorm 不同，不依赖 batch）；</li><li>作用：<ul><li>稳定隐藏层分布；</li><li>加速训练；</li><li>减少对初始化和学习率的敏感性。</li></ul></li></ul><hr><h3 id=-在-transformer-中的具体位置>📦 在 Transformer 中的具体位置：</h3><h4 id=encoder-层结构重复-n-次>Encoder 层结构（重复 N 次）：</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Input
</span></span><span class=line><span class=cl>  │
</span></span><span class=line><span class=cl>  ├─→ [Multi-Head Self-Attention] ──→ Add &amp; Norm ──→
</span></span><span class=line><span class=cl>  │
</span></span><span class=line><span class=cl>  └─→ [Feed Forward Network] ───────→ Add &amp; Norm ──→ Output
</span></span></code></pre></td></tr></table></div></div><h4 id=decoder-层结构重复-n-次>Decoder 层结构（重复 N 次）：</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Input
</span></span><span class=line><span class=cl>  │
</span></span><span class=line><span class=cl>  ├─→ [Masked Multi-Head Self-Attention] ──→ Add &amp; Norm ──→
</span></span><span class=line><span class=cl>  │
</span></span><span class=line><span class=cl>  ├─→ [Multi-Head Cross-Attention] ─────────→ Add &amp; Norm ──→
</span></span><span class=line><span class=cl>  │
</span></span><span class=line><span class=cl>  └─→ [Feed Forward Network] ───────────────→ Add &amp; Norm ──→ Output
</span></span></code></pre></td></tr></table></div></div><blockquote><p>⚠️ 注意：原始论文中是 <strong>先做子层计算 → 再 Add → 再 Norm</strong>（Post-LN）。<br>但现在很多变体（如 Pre-LN）会把 LayerNorm 放在子层<strong>之前</strong>，训练更稳定。</p></blockquote><hr><h3 id=-举个简单例子数值示意>🧠 举个简单例子（数值示意）：</h3><p>假设某层输入是向量 <code>x = [1.0, 2.0]</code>，经过自注意力后输出 <code>y = [0.8, 2.2]</code>。</p><ol><li><p><strong>Add（残差）</strong>：<br><code>z = x + y = [1.0+0.8, 2.0+2.2] = [1.8, 4.2]</code></p></li><li><p><strong>Norm（LayerNorm）</strong>：<br>对 <code>[1.8, 4.2]</code> 做归一化（减均值、除标准差 + 可学习缩放/偏移），得到标准化后的向量，比如 <code>[−1, 1]</code>（简化示意）。</p></li></ol><p>这个结果就作为下一层的输入。</p><hr><h3 id=为什么叫-add--norm-而不是-norm--add>❓为什么叫 “Add & Norm” 而不是 “Norm & Add”？</h3><ul><li>因为<strong>顺序是先 Add，再 Norm</strong>（在原始 Transformer 中）；</li><li>虽然名字是 “Add & Norm”，但实际是两个操作的组合，不是并行的。</li></ul><hr><h3 id=-总结>✅ 总结：</h3><table><thead><tr><th style=text-align:left>组件</th><th style=text-align:left>作用</th></tr></thead><tbody><tr><td style=text-align:left><strong>Add</strong></td><td style=text-align:left>残差连接，保留原始信息，缓解梯度消失</td></tr><tr><td style=text-align:left><strong>Norm</strong></td><td style=text-align:left>层归一化，稳定训练，加速收敛</td></tr><tr><td style=text-align:left><strong>Add & Norm</strong></td><td style=text-align:left>Transformer 的标准子层后处理模块，提升模型深度和性能</td></tr></tbody></table><p>它是 Transformer 能堆叠多层（如 6 层、12 层甚至更多）而依然可训练的关键设计之一！</p><p>如果你看到图中每个方框（Attention / FFN）后面都跟着一个 “Add & Norm”，现在你就知道它在做什么啦 😊</p><h2 id=mlpmulti-layer-perceptron多层感知机>MLP（Multi-Layer Perceptron，多层感知机）</h2><p>MLP（Multi-Layer Perceptron，多层感知机）和 FFN（Feed-Forward Network，前馈网络）这两个术语在深度学习中经常出现，它们密切相关，但在不同上下文中可能有细微差别。</p><h3 id=1-mlp多层感知机>1. <strong>MLP（多层感知机）</strong></h3><ul><li>MLP 是最早、最经典的神经网络结构之一。</li><li>它由**多个全连接层（Dense Layer）**堆叠而成，通常包括：<ul><li>输入层</li><li>一个或多个<strong>隐藏层</strong>（每个隐藏层后常接非线性激活函数，如 ReLU）</li><li>输出层</li></ul></li><li>特点：<strong>前馈</strong>（无环、无反馈）、<strong>全连接</strong>、<strong>非线性激活</strong>。</li><li>用途：可用于分类、回归等任务。</li></ul><h3 id=2-ffn前馈网络>2. <strong>FFN（前馈网络）</strong></h3><ul><li>FFN 是一个更广义的术语，泛指<strong>所有前馈结构的神经网络</strong>（即信号单向从前向后传播，不形成循环）。</li><li><strong>MLP 是 FFN 的一个特例</strong>。</li><li>在现代 Transformer 架构中，&ldquo;FFN layer&rdquo; 通常指一个<strong>两层全连接网络</strong>：<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>FFN(x) = max(0, xW₁ + b₁)W₂ + b₂
</span></span></code></pre></td></tr></table></div></div>即：线性变换 → ReLU（或 GELU）→ 线性变换。</li><li>这种 FFN 层<strong>结构上就是一个小型 MLP（通常只有1个隐藏层）</strong>，嵌入在 Transformer 的每个 Encoder/Decoder block 中。</li></ul><hr><h3 id=主要区别总结>主要区别总结：</h3><table><thead><tr><th style=text-align:left>方面</th><th style=text-align:left>MLP</th><th style=text-align:left>FFN（特指 Transformer 中的 FFN 层）</th></tr></thead><tbody><tr><td style=text-align:left>范畴</td><td style=text-align:left>一种具体的网络结构</td><td style=text-align:left>一类网络（广义）或一个模块（狭义）</td></tr><tr><td style=text-align:left>结构</td><td style=text-align:left>可含多个隐藏层</td><td style=text-align:left>通常只有<strong>1个隐藏层</strong>（两层线性）</td></tr><tr><td style=text-align:left>使用场景</td><td style=text-align:left>独立模型（如分类器）</td><td style=text-align:left>作为<strong>子模块</strong>嵌入大模型（如 Transformer）</td></tr><tr><td style=text-align:left>激活函数</td><td style=text-align:left>ReLU、tanh 等</td><td style=text-align:left>ReLU 或 GELU（Transformer 常用）</td></tr></tbody></table><blockquote><p>简单说：在 Transformer 中所说的 &ldquo;FFN layer&rdquo;，本质上就是一个<strong>浅层 MLP</strong>（通常2层），而 MLP 本身也可以看作是由多个 FFN 层堆叠而成的网络。</p></blockquote><p>如果你在阅读 Transformer 相关论文，看到 “FFN”，基本等价于一个带激活函数的两层全连接网络，这是 MLP 的一个简化特例。</p><h2 id=参考>参考</h2><ul><li><a href=https://transformers.run/c1/transformer/#transformer-%E7%9A%84%E7%BB%93%E6%9E%84 target=_blank rel="noopener noreffer">Transformer 模型</a></li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2026-01-31&nbsp;<a class=git-hash href=https://github.com/pigLoveRabbit520/commit/6e8b269e060494feb5892a805dfdc0ffda963781 target=_blank title="commit by rabbit520(energy3456789@gmail.com) 6e8b269e060494feb5892a805dfdc0ffda963781: Update attention dimensions from BSND to BNSD">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>6e8b269</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/triton_attention/index.md target=_blank>阅读原始文档</a></span></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/triton_learning/ class=prev rel=prev title=Triton写简单算子><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Triton写简单算子</a>
<a href=/verilog_learning/ class=next rel=next title=Verilog学习>Verilog学习<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.134.2">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2019 - 2026</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/rabbitLove520 target=_blank>pigLoveRabbit</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:50},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{algoliaAppID:"PASDMWALPK",algoliaIndex:"index.zh-cn",algoliaSearchKey:"b42948e51daaa93df92381c8e2ac0f93",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>